{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WW4t3iCYKuy"
      },
      "source": [
        "## 과제 1\n",
        "\n",
        "### **Q. 각 모델이 충족하는 속성에 대해 아래 표를 O/X로 채워주세요.**\n",
        "\n",
        "📍5번째 속성은 **LSTM 기준으로** O/X 여부 판단해주세요 ! <br>\n",
        "📍정답은 과제 마감 다음날 (9월 11일 수요일)에 **노션-정규세션-NLP basic**에 업로드 예정\n",
        "\n",
        "\n",
        "> #### **속성 설명**\n",
        "1. Order matters : 입력 시퀀스의 순서 중요 여부\n",
        "2. Variable Length : 고정된 길이가 아닌 다양한 길이의 시퀀스를 처리할 수 있는지 여부\n",
        "3. Differentiable : 미분가능\n",
        "4. Pairwise encoding : 두 단어 사이의 관계를 표현\n",
        "5. Preserves long-term : 장기적인 의존성\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paUeOH0OYNU0"
      },
      "source": [
        "|               | N-gram | RNN   | LSTM  | Transformer |\n",
        "|:-------------:|:------:|:-----:|:-----:|:-----------:|\n",
        "| Order matters |        |     |      | O           |\n",
        "| Variable length |      |     |      | O           |\n",
        "| Differentiable |       |     |      | O           |\n",
        "| Pairwise encoding |    |     |      | O           |\n",
        "| Preserves long-term |  |     |      | O           |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14MthA8WYQev"
      },
      "source": [
        "## 과제 2\n",
        "\n",
        "\n",
        "### 목표 : 독일어를 영어로 번역하는 모델 만들기\n",
        "독일어 문장을 입력하면 영어로 번역해주는 모델을 seq2seq로 구현해봅시다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PbwGzED6TIV",
        "outputId": "6a978041-4e30-465e-e092-400c352c5c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext==0.6.0 in /opt/anaconda3/lib/python3.11/site-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (from torchtext==0.6.0) (2.4.1)\n",
            "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchtext==0.6.0) (1.26.4)\n",
            "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.11/site-packages (from torchtext==0.6.0) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (2024.2.2)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (2023.10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPKSSHzQ6Uoh",
        "outputId": "eba95cb8-9885-4d76-b5fc-3327882af216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /opt/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.3.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.0)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
            "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
            "Collecting de-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from de-core-news-sm==3.7.0) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.3.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.1.3)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.0)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDcmbrWMhRJ7",
        "outputId": "fee09225-71e8-43a1-fd8f-83168c2e21ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /opt/anaconda3/lib/python3.11/site-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (68.2.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /opt/anaconda3/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.3)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /opt/anaconda3/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJWiAS2yWutF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import spacy\n",
        "from torchtext.datasets import TranslationDataset\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHmIKputmJfU"
      },
      "source": [
        "### Tokenizers\n",
        "\n",
        "- 문장의 토큰화, 태깅 등의 전처리를 수행하기 위해 `spaCy` 라이브러리에서 영어와 독일어 전처리 모듈을 설치해줍니다.\n",
        "- 두 언어의 문장이 주어졌기 때문에 영어와 독일어 각각에 대해 전처리해주어야 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7ZtI5IXm7EG"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co1TC8yv7yrX",
        "outputId": "b7b82b77-86d7-4f13-eeb7-e4815afc988c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인덱스 0: I\n",
            "인덱스 1: am\n",
            "인덱스 2: a\n",
            "인덱스 3: student\n",
            "인덱스 4: .\n"
          ]
        }
      ],
      "source": [
        "# 예시\n",
        "result = spacy_en.tokenizer(\"I am a student.\")\n",
        "\n",
        "for i, token in enumerate(result):\n",
        "    print(f\"인덱스 {i}: {token.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEGmP9Uk8gQG"
      },
      "source": [
        "필드(field) 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMjoI1XE7tGF"
      },
      "outputs": [],
      "source": [
        "#===================================================\n",
        "# 💡 토큰화 결과가 list로 반환될 수 있도록 return 결과값을 채워주세요\n",
        "# seq2sxeq 논문에 의하면, input 단어의 순서를 바꾸면 최적화가 더 쉬워져 성능이 좋아진다고 합니다.\n",
        "# 💡 독일어 토큰화 결과가 역순으로 return될 수 있도록 반영해주세요!\n",
        "#===================================================\n",
        "def tokenize_de(text):\n",
        "    return\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3wGw1nPnpMd"
      },
      "source": [
        "필드(field) 라이브러리를 이용해 데이터셋에 대한 구체적인 전처리 내용을 명시해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubKI59GPpQ1f"
      },
      "outputs": [],
      "source": [
        "# 독일어\n",
        "SRC = Field(tokenize= tokenize_de, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
        "# 영어\n",
        "TRG = Field(tokenize= tokenize_en, init_token = '<sos>', eos_token = '<eos>', lower = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_ccI6_-8hR3"
      },
      "source": [
        "### 데이터 불러오기\n",
        "\n",
        "대표적인 영어-독어 번역 데이터셋 Multi30k을 불러옵니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLA5kXAAf2uw",
        "outputId": "b38c5e3e-8721-4ad5-d6ef-efb66d47407a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'dataset'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 313 (delta 17), reused 21 (delta 16), pack-reused 281 (from 1)\u001b[K\n",
            "Receiving objects: 100% (313/313), 18.21 MiB | 10.81 MiB/s, done.\n",
            "Resolving deltas: 100% (69/69), done.\n",
            "gunzip: can't stat: /content/dataset/data/task1/raw/train.de.gz (/content/dataset/data/task1/raw/train.de.gz.gz): No such file or directory\n",
            "gunzip: can't stat: /content/dataset/data/task1/raw/train.en.gz (/content/dataset/data/task1/raw/train.en.gz.gz): No such file or directory\n",
            "gunzip: can't stat: /content/dataset/data/task1/raw/val.de.gz (/content/dataset/data/task1/raw/val.de.gz.gz): No such file or directory\n",
            "gunzip: can't stat: /content/dataset/data/task1/raw/val.en.gz (/content/dataset/data/task1/raw/val.en.gz.gz): No such file or directory\n",
            "gunzip: can't stat: /content/dataset/data/task1/raw/test_2018_flickr.de.gz (/content/dataset/data/task1/raw/test_2018_flickr.de.gz.gz): No such file or directory\n",
            "gunzip: can't stat: /content/dataset/data/task1/raw/test_2018_flickr.en.gz (/content/dataset/data/task1/raw/test_2018_flickr.en.gz.gz): No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/multi30k/dataset.git\n",
        "\n",
        "# 압축해제\n",
        "!gunzip /content/dataset/data/task1/raw/train.de.gz\n",
        "!gunzip /content/dataset/data/task1/raw/train.en.gz\n",
        "!gunzip /content/dataset/data/task1/raw/val.de.gz\n",
        "!gunzip /content/dataset/data/task1/raw/val.en.gz\n",
        "!gunzip /content/dataset/data/task1/raw/test_2018_flickr.de.gz\n",
        "!gunzip /content/dataset/data/task1/raw/test_2018_flickr.en.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchtext -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvY2cxXgk6MI",
        "outputId": "2c24d69e-f49c-4d39-d216-5cfdebe637a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.4.0\n",
            "Uninstalling torch-2.4.0:\n",
            "  Successfully uninstalled torch-2.4.0\n",
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1 torchtext==0.10.0"
      ],
      "metadata": {
        "id": "V61tsoMSk8OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0JchGVU91Q5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from torchtext.data import Field, BucketIterator\n",
        "from torchtext.datasets import TranslationDataset\n",
        "\n",
        "# 필드 정의 (SRC는 소스 언어, TRG는 대상 언어)\n",
        "SRC = Field(tokenize=str.split, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "TRG = Field(tokenize=str.split, lower=True, init_token='<sos>', eos_token='<eos>')\n",
        "\n",
        "# 데이터 경로 설정\n",
        "data_path = '/content/stopwords.txt'\n",
        "\n",
        "# TranslationDataset을 사용하여 데이터셋 생성\n",
        "train_data = TranslationDataset(path=data_path, exts=('train.de', 'train.en'), fields=(SRC, TRG))\n",
        "val_data = TranslationDataset(path=data_path, exts=('val.de', 'val.en'), fields=(SRC, TRG))\n",
        "test_data = TranslationDataset(path=data_path, exts=('test_2018_flickr.de', 'test_2018_flickr.en'), fields=(SRC, TRG))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxLylN1y-urW"
      },
      "outputs": [],
      "source": [
        "print(f\"학습 데이터셋(training dataset) 크기: {len(train_data.examples)}개\")\n",
        "print(f\"평가 데이터셋(validation dataset) 크기: {len(val_data.examples)}개\")\n",
        "print(f\"테스트 데이터셋(testing dataset) 크기: {len(test_data.examples)}개\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpt6l_Xd_AQX",
        "outputId": "f8d29c4a-bef3-4d61-8175-fc08ca298369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'src': ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n",
            "{'src': ['.', 'antriebsradsystem', 'ein', 'bedienen', 'schutzhelmen', 'mit', 'männer', 'mehrere'], 'trg': ['several', 'men', 'in', 'hard', 'hats', 'are', 'operating', 'a', 'giant', 'pulley', 'system', '.']}\n"
          ]
        }
      ],
      "source": [
        "print(vars(train_data.examples[0]))\n",
        "print(vars(train_data.examples[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNoigj40AD_0"
      },
      "source": [
        "- `build_vocab`함수를 이용하여 영어와 독일어의 단어 사전을 생성해줍니다. 이를 통해 각 token이 indexing됩니다\n",
        "- 단, vocabulary는 훈련 데이터셋에 대해서만 만들어져야 합니다.\n",
        "- `min_freq`를 사용하여 최소 2번 이상 나오는 단어들만 사전에 포함되도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfK-pAr_ApLP"
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyaA5P0Mrqaa"
      },
      "outputs": [],
      "source": [
        "print(TRG.vocab.stoi[\"abcabc\"]) # 없는 단어: 0\n",
        "print(TRG.vocab.stoi[TRG.pad_token]) # 패딩(padding): 1\n",
        "print(TRG.vocab.stoi[\"\"]) # : 0\n",
        "print(TRG.vocab.stoi[\"\"]) # : 0\n",
        "print(TRG.vocab.stoi[\"hello\"])\n",
        "print(TRG.vocab.stoi[\"world\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmHXb1phBXJN"
      },
      "source": [
        "- 시퀀스 데이터는 각 문장의 길이가 다를 수 있습니다.\n",
        "- `BucketIterator는` 유사한 길이를 가진 샘플들을 같은 배치에 묶어주는 역할을 하기 때문에, 고정된 길이로 맞추기 위한 패딩의 양을 최소화할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uazI6xuv8rDH"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z154iai8Czsr"
      },
      "source": [
        "- 첫 번째 배치를 출력한 결과, [sequence length, batch size]라는 tensor가 생성됩니다\n",
        "- `sequence length`는 해당 배치 내에서 가장 긴 문장의 길이를 의미하며, 이보다 짧은 문장은 <pad> token으로 채워집니다.\n",
        "- 편의상 transpose한 뒤, 첫 번째와 두 번째 문장의 텐서를 출력하면, 특정 단어에 대응하는 인덱스가 출력되는 것을 알 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r-eReL8rwm_"
      },
      "outputs": [],
      "source": [
        "for i, batch in enumerate(train_iterator):\n",
        "    src = batch.src\n",
        "    trg = batch.trg\n",
        "\n",
        "    print(f\"첫 번째 배치의 text 크기: {src.shape}\")\n",
        "    src = src.transpose(1,0)\n",
        "    print(src[0])\n",
        "    print(src[1])\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51xSGy35XLvG"
      },
      "source": [
        "### Building the Seq2Seq with LSTM Model\n",
        "\n",
        "- seq2seq 이해를 위한 과제이니, 아래를 참고하여 작성해도 무방합니다 :)\n",
        "\n",
        "\n",
        "https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Sequence_to_Sequence_with_LSTM_Tutorial.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9WWS97vYnSb"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtpU_ZjeYNEZ"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡아래줄에 embedding과 multi-layer LSTM 부분을 채워주세요 (dropout 포함)\n",
        "        #=========================================#\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)  # input_dim 크기의 임베딩 레이어 생성\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=p)  # 다층 LSTM 생성\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [x length, batch size]\n",
        "        embedding = self.dropout(self.embedding(x))  # embedding = [x length, batch size, emb size]\n",
        "\n",
        "        outputs, (hidden, cell) = self.rnn(embedding)\n",
        "\n",
        "        # hidden = [n layers, batch size, hid dim]\n",
        "        # cell = [n layers, batch size, hid dim]\n",
        "        # outputs = [src len, batch size, hid dim]\n",
        "\n",
        "        return hidden, cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrQoLPg-Ype1"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOVQfminYknh"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡아래 코드를 채워주고, 각각 어떤 역할을 하는지 주석으로 간단히 설명해주세요\n",
        "        #\n",
        "        #=========================================#\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)     # 입력된 단어를 임베딩 벡터로 변환\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=p)  # LSTM으로 임베딩 벡터 처리\n",
        "        self.fc = nn.Linear(hid_dim, output_dim)  # LSTM 결과를 출력 차원으로 변환\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "\n",
        "        # 현재 input 형태 = [batch size]\n",
        "        # Decoder는 한번에 하나의 토큰만 처리하도록 sequence length = 1이 되어야 합니다\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        embedding = self.dropout(self.embedding(input))\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡self.rnn() 괄호 안 부분을 채워주세요\n",
        "        #=========================================#\n",
        "        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
        "\n",
        "        prediction = self.fc(output.squeeze(0))  #prediction = [batch size, output dim]\n",
        "\n",
        "        return prediction, hidden, cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNsTqRamcfPg"
      },
      "source": [
        "### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik28Umx6gAPW"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        #=========================================#\n",
        "        # 💡trg를 사용하여 decoder에 입력할 첫번째 input을 설정해주세요\n",
        "        #=========================================#\n",
        "        input = trg[0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "\n",
        "            outputs[t] = output\n",
        "\n",
        "            # predictions들 중에 가장 잘 예측된 token 추출\n",
        "            best_guess = output.argmax(1) # [batch size]\n",
        "\n",
        "            input = trg[t] if random.random() < teacher_forcing_ratio else best_guess\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfKrIZ63jkVF"
      },
      "source": [
        "### **Q. 위 코드에서는 매 시점마다 확률이 가장 높은 다음 단어를 선택하는 Greedy decoding  방식이 사용됩니다. 이런 방법을 채택할 경우 발생할 수 있는 문제점은 무엇일지 작성해주세요.**\n",
        "\n",
        "\n",
        "```python\n",
        "\n",
        "# predictions들 중에 가장 잘 예측된 token 추출\n",
        "best_guess = output.argmax(1) # [batch size]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "➡️  **Q. 위 코드에서는 매 시점마다 확률이 가장 높은 다음 단어를 선택하는 Greedy decoding 방식이 사용됩니다. 이런 방법을 채택할 경우 발생할 수 있는 문제점은 무엇일지 작성해주세요.**\n",
        "\n",
        "**Greedy Decoding 방식의 문제점**:\n",
        "\n",
        "1. **전체 문장의 최적 해를 보장하지 않음**:  \n",
        "   Greedy Decoding은 매 시점마다 가장 높은 확률의 단어를 선택하지만, 전체 문장에서 최적의 결과를 보장하지 않습니다. 개별적인 단계에서는 좋은 선택일 수 있지만, 전체 문맥을 고려할 때는 더 나은 선택을 놓칠 수 있습니다.\n",
        "\n",
        "2. **글로벌 최적화 실패**:  \n",
        "   Greedy Decoding은 단기적인 선택에만 의존하기 때문에, 문장 전체의 의미를 고려하지 못하고 글로벌 최적화에 실패할 가능성이 있습니다. 결과적으로 의미가 어색하거나 잘못된 문장이 생성될 수 있습니다.\n",
        "\n",
        "3. **모호한 상황에서의 한계**:  \n",
        "   여러 후보 단어들이 비슷한 확률을 가질 때, 가장 높은 확률만을 선택하는 Greedy Decoding은 다른 유사한 후보를 무시할 수 있어 중요한 문맥을 놓치는 결과를 초래할 수 있습니다.\n",
        "\n",
        "4. **다양성 부족**:  \n",
        "   Greedy Decoding은 항상 가장 높은 확률의 단어만을 선택하기 때문에, 생성되는 문장이 다양하지 않고 단조로울 수 있습니다. 동일한 입력에 대해 항상 동일한 결과를 생성하게 되어 다양성이 부족해집니다.\n",
        "\n",
        "**대안**:\n",
        "이 문제를 해결하기 위한 대안으로는 **Beam Search**와 같은 탐색 알고리즘을 사용할 수 있습니다. Beam Search는 여러 후보 문장을 동시에 고려하여 더 높은 확률을 가진 문장을 선택할 수 있게 해줍니다. 이를 통해 Greedy Decoding의 단기적인 최적화 문제를 완화할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHk36-bkh055"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj33q4Izh3aB"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoFprkiyh5vm"
      },
      "source": [
        "모델 초기 가중치 값은 논문의 내용대로 U(−0.08,0.08)의 연속균등분포로부터 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oLZ0jXyh4zG"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpKWiiBIF5NA"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# 뒷 부분의 패딩(padding)에 대해서는 값 무시\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veJrgUHBjRVa"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDfEPQUwFvVG"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0fOgC3jc8K4"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output = model(src, trg, 0)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OCHWUhmGORD"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0VHYyZLjFO2"
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 3\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCou2VSKVz64"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('/content/seq2seq-lstm-model.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}